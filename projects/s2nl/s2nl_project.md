## Signal to Noise Loops<br/>

#### Project Overview<br/>
Signal to Noise Loops emerged from a broader project entitled 'Auditory Display for Large-scale IoT Networks' carried out at the CONNECT Centre Trinity College Dublin. The Signal to Noise Loops project links live electronic music performance, IoT Network Data, and Generative Music techniques. Each performance draws data from networks of IoT devices placed around Dublin City. Sensor and network data are mapped to control parameters of the live performance. How this takes place is mediated by a generative music system. The mood or affective state of the system is determined by the state of Dublin city, as represented through the IoT sensor data. The system's mood in turn determines the musical choices it makes while improvising alongside a human performer. Each performance with the system is unique as it represents a complex array of data relations that describe the state of Dublin City and any given time. The project involved the iterative development of the system with each performance acting as an evaluation after which the system would be expanded and further refined. <br />

#### Iteration 1: Evolutionary Algorithms & Direct Mapping <br/>
The first iteration of the system introduced evolutionary approaches to generative music influenced by Eduardo Reck Miranda and Alan Dorin. The evolutionary algorithms generated and organised content within the performance at the textural and timbral levels. A variety of cellular automata rulesets (including Conway's Life, Replicator by Fredkin, Gnarl by Evans, and Serviettes) are used to control the application of buffer effects, pitch modulation, and distortion. The degree to which these are applied is determined by the overall noise levels at different monitors around the city.
The system reacts to a live improvisation by an instrumental performer, in this case, an electric guitarist, in real-time. Alongside the evolutionary component, the state of the city as represented in the noise data directly determined a series of sonic parameters at the timbral, textural, and macro organisational levels throughout a given performance.
The bulk of the is written in Python and it is used to extend the capabilities of Ableton Live to leverage and map the IoT data. This early version of the system grew out of work undertaken to sonify IoT network data from several sources at CONNECT, the Science Foundation Ireland Research Centre for Future Networks headquartered at Trinity College Dublin.

#### Performance 1: Sonic Dreams 2017<br/>
The first use for the system was for the piece 'Noise Loops for Laptop, Improvised Electric Guitar and Dublin City Noise Data'. This was performed at the 2017 Sonic Dreams Festival. In this piece, IoT data from sensors measuring ambient noise levels around Dublin city was mapped to control the performance parameters of a live electric guitar improvisation. The data was mapped to drive the application of the evolutionary algorithms and to directly control the timbre of the guitar utilizing a multiband distortion to morph the sound. The data was also mapped to control advanced buffer, delay, and filtering processing of the performance and that also controlled the synthesis of percussive elements within the performance. In this iteration, the system was mapped to control the live DSP process which mashed up and remixed the performance in real-time based on the IoT data. <br />
<br />

<img src="images/SonicDreams.jpg?raw=true"/>
* [Sonic Dreams 2017 Poster](images/SonicDreams.jpg)

<iframe seamless="" src="https://bandcamp.com/EmbeddedPlayer/album=793201642/size=large/bgcol=ffffff/linkcol=0687f5/tracklist=false/artwork=small/track=2969131966/transparent=true/" style="border: 0; height: 120px; width: 100%;"><a href="http://stephenroddy.bandcamp.com/album/home-part-2-silence-ep">Home Part 2: Silence EP by Stephen Roddy</a></iframe>

<br />
<div style="text-align: center;">
<span style="font-weight: normal;">
Dublin City Noise Loops&nbsp;</span></div>
<div style="text-align: center;">
<br /></div>

#### Iteration 2: Generative Systems & Musical Interactions<br/>
The second iteration of the system involved procedural approaches to generative music informed by the works of Brian Eno, Steve Reich, and Terry Riley. It generated harmonic and sonic materials at the level of the individual object/note/event. The mode of musical interaction here was changed also. The system no longer reacted to an electric guitar performance but instead reacted to improvised electronic music generated by the performer. This system employed liine’s Lemur app for iOS to control the synthesis of audio materials in Native Instrument’s Reaktor and the Ableton Live Wavetable Synth. Reaktor also ran patches employing a mixture of additive and subtractive synthesis techniques to generate audio materials. This opened up a wealth of new possibilities for mapping Iotdata to sonic parameters. The system was also expanded here to allow IoT data to be 'played back' at a variety of user-defined rates moving away from a purely real-time model for using the data. <br />

#### Performance 2: xCoAx 2018<br/>
The next performance with the system titled 'Signal to Noise Loops i++'' took place at xCoAx in Madrid in 2018. This performance involved the more refined version of the system which had been further developed after the SAW festival performance. The system was updated to generate music alongside the human performer. Essentially, the system would 'listen' to what the human performer played and then make decisions about what it wanted to play, and whether or not it wanted to intervene in the human’s performance. When the IoT data represented a healthy, happy, calm city the system would embellish the performance and when the data represented a more chaotic city, the system would disregard the performance replacing it with harsh sounding procedurally generated harmonic materials. I gave a talk describing how this iteration of the system worked at the conference and took part in a broader artists panel also.<br />

* [xCoAx Performance](images/xcoaxPerformance.jpg)
* [xCoAx Artist Talk](images/xcoax1.jpg)
* [xCoAx Artists Panel](images/xcoax5.jpg)

<img src="images/PosterXcoax.jpg?raw=true"/><br/>
* [xCoAx Poster](images/PosterXcoax.jpg)


#### Iteration 3: More Data<br/>
The third iteration of the system made use of new IoT data sources. The thinking here was to structure the system so that it could be said to 'make decisions based on the ‘mood’ or ‘affective state’ of Dublin City as represented in the wider set of IoT data sources. The system reads noise levels, pollution levels, traffic flows (pedestrian and vehicle), emergency warnings, and weather data and these data points define the affective state or mood of the generative system (both the evolutionary and procedural components), and data to sound mapping strategy, and thereby the music. <br />
When the data represents a healthy and functioning city the generative system is in a good mood and collaborates better with the human performer in a harmonious manner, coordinating its music-making with that of the human. When the city is in a sub-optimal state, the system has a 'negative affective state' or 'mood' and begins to overwrite the human performer and make more independent musical decisions that are reflective of the state of the city.<br />

#### Performances 3 & 4: ISSTA & CSMC 2018<br/>
A third performance took place at ISSTA 2018 in Derry/Londonderry and a fourth performance took place at CSMC 2018 in Dublin. In these performances, the wider set of data sources described above are mapped to control a range of sound synthesis, timbral and performance parameters in the piece, as well as interacting with the procedural and evolutionary components of the generative system described previously. <br />

<img src="images/csmc2018_concert_performers.jpg.webp?raw=true"/><br/>
* [CSMC2018 Concert Performance](/images/csmc2018_concert_performers.jpg.webp)

* [ISSTA 2018 Programme](/files/ISSTA programme.pdf)
* [CSMC 2018 Concert](files/Concert – CSMC2018.pdf)

<br />
<div style="text-align: center;">
<iframe seamless="" src="https://bandcamp.com/EmbeddedPlayer/album=793201642/size=large/bgcol=ffffff/linkcol=0687f5/tracklist=false/artwork=small/track=3477147735/transparent=true/" style="border: 0; height: 120px; width: 100%;"><a href="http://stephenroddy.bandcamp.com/album/home-part-2-silence-ep">Home Part 2: Silence EP by Stephen Roddy</a></iframe>Signal to Noise Loops i++ </div>
<br />

#### Iteration 4: Machine Learning
While the earlier systems used procedural techniques alone to generate materials at the level of the note/object/event version 4 integrates machine learning tools alongside procedural techniques. The system uses Google Magenta's 'Continue' tool (using RNNs) to elaborate upon the data-driven, procedurally generated harmonic materials. It also uses 'Interpolate' (driven by MusicVAE) to generate a series of melodies with qualities lying along a linear spectrum <i>between</i> two input melodic sequences.

#### Online Pivot: COVID-19 Crisis Response <br />
While working on the fourth iteration of the system the COVID-19 Crisis hit. To continue the project, I chose to adapt it to an online mode of presentation. I also chose to make use of data representing the city before the COVID-19 crisis hit as well as during the COVID-19 crisis. This involved using the new iteration of the system to create a fixed composition in two movements. In the interests of clarity, I used one stream of data, the original Noise Data. The first movement used data from March of 2019 while the second movement used data from March 2020.
Alongside the piece, I also created a visualisation of the data which maps the Noise data to control the radii and colours of dots in a dot-matrix representation of Dublin City.


#### Performance 5: The New York electroacoustic Music festival
The piece is due to be performed at the [2021 New York Electroacoustic Music Festival](https://nycemf.org/).

<iframe width="560" height="315" src="https://www.youtube.com/embed/f5yggfFRPAA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

[Youtube Link](https://www.youtube.com/watch?v=f5yggfFRPAA)

<iframe frameborder="0" src="https://drive.google.com/file/d/17JsG6ejsXp0xaS7fLn68oP-1kyStzXYM/preview" style="height: 500px; width: 600px;"></iframe>

[NYCEMF2021 Concert Program](/files/2021-Program-Book.pdf)


#### Performance 6: International Conference on Computer Music
The piece has also been performed (music only) at the International Conference on Computer Music at Santiago Chile on July 25th 2021.

<img src="images/ICMC Santiago Chile 2021.png?raw=true"/><br/>


### Discussion <br />
The point of mapping data to sound, and more specifically IoT data is to leverage some of the interesting patterns that present themselves across data streams/sets of this manner. Data-driven music is different from sonification where the point is to faithfully communicate or represent the data to the listener. Data-driven music is closer in many ways to algorithmic music composition than it is to sonification because of its focus on finding patterns in the data that might be interesting when mapped to sonic and musical parameters. My previous data-driven music work has employed algorithmic composition techniques and dealt with used from the global financial crash. More recently I have begun to work with IoT data as I believe that the kinds of data we choose to measure and our reasons for measuring them say a lot about what a society values, cares about, and finds interesting while the specific data measurements chronicle the complex interactions between people, the technologies they create and the worlds in which those people and technologies are situated. <br />
<br />
While these explicit points of information may not be directly represented in a performance the rich interleaved patterns of interaction between people, place and technology are transposed into the sonic realm in each performance. While more abstract and implicit in nature it is the aesthetic dimensionality of these interlocked patterns, which is of interest to me. <br />
<br />
<h4>
Paper on earlier iteration of system:</h4>
<br />
<iframe frameborder="0" src="https://drive.google.com/file/d/1awK-N8_hdNML3_gH56VgX2g1jtuekjO6/preview" style="height: 500px; width: 600px;"></iframe>

<br />

### Outputs & Activities

### Performances:
- [Signal to Noise Loops 3++ @ ISSTA 2018, Derry, September 2018](http://issta.ie/call-2018/)
- [Signal to Noise Loops i2+: Noise Water Dirt @ CSMC 2018, Dublin, August 2018](https://csmc2018.wordpress.com/)
- [Signal to Noise Loops i++ Live @ xCoAx 2018, Madrid](https://2018.xcoax.org/#perf04)
- [Noise Loops for Laptop, Improvised Electric Guitar and Dublin City Noise Level Data @ Sonic Dreams 2017, Sonic Arts Waterford, September 30th 2017](https://1.bp.blogspot.com/-HhZc6oL93Og/W0yMNH4jnVI/AAAAAAAAGFE/VlxW3bOMTlono3rkqbBMtE4XAxElOOgAQCLcBGAs/s1600/Sonic-Dreams-Festival-2017-final-poster-2.jpg)

### Recordings
<h4>Link to Performances with earlier iterations of the System:</h4>
Dublin City Noise Loops<br />
<a href="https://open.spotify.com/track/63x9Nav3h61MNbcV6uycCX">https://open.spotify.com/track/63x9Nav3h61MNbcV6uycCX</a><br />
<a href="https://music.apple.com/us/album/dublin-city-noise-loops/1450892433?i=1450892435">https://music.apple.com/us/album/dublin-city-noise-loops/1450892433?i=1450892435</a><br />
<a href="https://stephenroddy.bandcamp.com/track/dublin-city-noise-loops">https://stephenroddy.bandcamp.com/track/dublin-city-noise-loops</a><br />
<br />
Signal to Noise Loops i++<br />
<a href="https://open.spotify.com/track/5B4bh4fpgR9mFqv3OgKDRs">https://open.spotify.com/track/5B4bh4fpgR9mFqv3OgKDRs</a><br />
<a href="https://music.apple.com/us/album/signal-to-noise-loops-i/1450892433?i=1450892434">https://music.apple.com/us/album/signal-to-noise-loops-i/1450892433?i=1450892434</a><br />
<a href="https://stephenroddy.bandcamp.com/track/signal-to-noise-loops-i">https://stephenroddy.bandcamp.com/track/signal-to-noise-loops-i</a><br />
<br />

<!--

### Creative Skills
Sound Design. Music Composition. Live Electronic Music performance. Instrumental Guitar Performance.  Audio Sound Art. Visual Design.

### Technical
IoT Networks. Statistical Data Analytic. Python. HTTP, OSC & MIDI protocols. Evolutionary Computing. Audio DSP. Creative Coding. HTTP. HCI. Auditory Display & Sonification. Audio Engineering.

-->

### Tags
Signal to Noise Loops. Generative Music. Music. Data. Machine Learning. Evolutionary Algorithms.
